# Docker 101

---

*These explanations rely heavily on Nigel Poulton's [Docker Deep Dive book](https://amzn.to/2Jr4gtg), 
so if you prefer reading from the source I totally recommend the book, it is awesome.*

---

Once upon a time, each new application had to run on a separate (physical) server. 
Every time there was a new application a new server had to be bought. And since each 
application should have had the room to scale a much more powerful server would be 
bought than needed. Needless to say, this was quite expensive.

Then Virtual Machines appeared, and they helped solve the issue through allowing for 
multiple applications to run each on a dedicated VM on the same server:

<img src="assets/images/docker_101_1.png">

*[from docker blog](https://www.docker.com/blog/containers-replacing-virtual-machines/)*

But... VMs are still heavy on resources, relatively hard to migrate and restart. Containers to the rescue!

Docker (most popular option for the containers), allows to use much more lightweight option, with great restart time and perfect portability. You can transport any container (or image it is build from) from any machine to another and be 100% that it will work just as well. And on top of that it is as lightweight as it gets.

<img src="assets/images/docker_101_2.png">

*[from docker blog](https://www.docker.com/blog/containers-replacing-virtual-machines/)*

An important difference is that VMs run on virtual hardware, where Hypervisor is responsible for communicating/scheduling operations to the real underlying OS and through it to its hardware, whereas Docker runs on the same Kernel as native OS. This allows for greater efficiency (and fantastic start times) but has potential dangers if containers are not well configured.

A simple example of such danger is that Docker daemon is running as root on the host so if an attacker manages to penetrate the container and escalate rights he might be able to essentially be acting as root in spite of being locked in an isolated container - so important takeaway is that containers might be isolated from each other but they are still running directly on the host - hence configuration matters.

## Docker under the hood

<img src="assets/images/docker_101_3.png">

*from Nigel Poulton's [Docker Deep Dive](https://amzn.to/2Jr4gtg) book*

- `Docker client` is usually a terminal from which you type your commands.
- `Docker daemon` is a background process that executes the commands.
- `containerd` is a container supervisor which is responsible for container execution logic (start | stop| pause ... ).
- `runc` is a container runtime and it gets created with each container (so only once contrainerd but many runc-es)

So when you type a command like docker run hello-world docker client converts it into the appropriate form and POSTs it to the endpoint (on Linux the socket is /var/run/docker.sock and on Windows it's \pipe\docker_engine) exposed by the Docker daemon. Once docker-engine receives a command it makes a call to containerd. containerd converts the required Docker image into an OCI bundle (Open Container Initiative - is a project of Linux foundation that sets container standards) and tells runc to create a new container. runc then interfaces with the OS kernel to make it happen. The container process is started as a child of runc and as soon as that happens runc exits. And that is the whole process.

Main advantage of decoupling of daemon engine from container logic is that now daemon engine can be stopped/upgraded/restarted without affecting running containers - huge benefit in production where you don't want to stop all running containers for each daemon update.

### shim

For the sake of not leaving out any details, I should mention a couple of words about shim. shim is there to take over being a parent process when runc exits. That keeps it light on the resources to not have to have as many runc-es as containers but we still need a parent process (in this case shim) for each container for small tasks like keeping STDIN and STDOUT open so that when the daemon restarts container is not affected and to report container's exit status back to the daemon.

So what is docker daemon for if the whole docker management is delegated to containerd and runc/shim? Docker daemon's responsibilities include image management, REST API, authentication, security, networking, and orchestration.

---

That's all folks. This was all the theory you would need to get the picture of how Docker works. I encourage you to play with it, because whether you are ready for it or not...

Cloud is coming!
